{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4854e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "os.environ[\"hadoop.home.dir\"] = \"C:\\\\hadoop\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WriteTest\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.local.dir\", \"C:/temp/spark-temp\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4860888",
   "metadata": {},
   "source": [
    "# Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff9624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+----------+-----------+----------+----------+--------------+------------------+--------------+-------------------+---------+------------------+-----------------+--------------------+---------+----------------+-----------------+\n",
      "|vehicle_spec_id|year|      make|      Model|drivetrain|max_torque|max_horsepower|max_horsepower_rpm|max_torque_rpm|engine_displacement|fuel_type|fuel_tank_capacity|fuel_economy_city|fuel_economy_highway|cylinders|forced_induction|device_generation|\n",
      "+---------------+----+----------+-----------+----------+----------+--------------+------------------+--------------+-------------------+---------+------------------+-----------------+--------------------+---------+----------------+-----------------+\n",
      "|        1000500|2016|     Honda|      Civic|         2|       174|           140|              6500|          1500|              1.799|     1059|                47|             16.5|                20.0|        4|            1054|                5|\n",
      "|        1000501|2016|      Jeep|    Compass|         4|       350|           171|              3750|          1250|              1.956|     1059|                60|             17.1|                21.2|        3|            null|                4|\n",
      "|        1000502|2016|   Hyundai|      Creta|         2|       260|           126|              4000|          1500|              1.582|     1059|                55|            19.67|                24.1|        4|            1054|                3|\n",
      "|        1000503|2016|     Skoda|     Superb|         2|       250|           177|              5100|          1750|              1.798|     1059|                66|            14.67|                23.3|        3|            null|                2|\n",
      "|        1000504|2017|Volkswagen|     Passat|         4|       350|           174|              3600|          1500|              1.968|     1059|                66|            17.42|               20.43|        4|            null|                4|\n",
      "|        1000506|2017|       BMW|3 Series GT|         2|       400|           188|              4000|          1800|              1.995|     1059|                57|            21.76|               26.32|        3|            null|                3|\n",
      "|        1000507|2017|      Audi|         Q3|         3|       250|           148|              5000|          1400|              1.395|     1059|                64|             16.9|                21.4|        3|            1054|                2|\n",
      "+---------------+----+----------+-----------+----------+----------+--------------+------------------+--------------+-------------------+---------+------------------+-----------------+--------------------+---------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This loads the CSV file and sets the column names to be the header values as well as infers the schema\n",
    "# We cache this as it will be re-used many times\n",
    "dfVehicle = spark.read.csv(\"C:/Users/tjsde/OneDrive/Documents/Git/tech-test-data/supporting-data/vehicle.csv\", header=True, inferSchema=True).cache()\n",
    "dfVehicle.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef43a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------+-------------------+--------+----------+------+-------+-------+------------------+--------+\n",
      "|trip_id|datetime           |vehicle_spec_id|engine_coolant_temp|eng_load|fuel_level|iat   |rpm    |lat    |long              |velocity|\n",
      "+-------+-------------------+---------------+-------------------+--------+----------+------+-------+-------+------------------+--------+\n",
      "|26     |2017-02-27 23:27:20|1000512        |90.0               |193.25  |117.0     |102.94|1913.08|30.9375|-87.10694444444444|52.28   |\n",
      "|26     |2017-02-27 22:55:07|1000512        |92.0               |200.33  |113.0     |109.58|1906.01|30.9375|-87.6438888888889 |52.2    |\n",
      "|26     |2017-02-27 23:23:48|1000512        |92.0               |197.69  |106.0     |96.66 |1915.43|30.9375|-87.16583333333334|64.09   |\n",
      "|26     |2017-02-27 23:17:53|1000512        |88.0               |203.73  |105.0     |103.58|1907.7 |30.9375|-87.26444444444445|28.66   |\n",
      "|26     |2017-02-27 23:09:58|1000512        |96.0               |193.82  |110.0     |111.74|1911.74|30.9375|-87.3963888888889 |54.85   |\n",
      "|26     |2017-02-27 23:21:04|1000512        |86.0               |204.56  |113.0     |108.74|1905.19|30.9375|-87.21138888888889|58.95   |\n",
      "|26     |2017-02-27 22:21:51|1000512        |96.0               |199.15  |108.0     |110.44|1917.34|30.9375|-88.19833333333334|69.46   |\n",
      "|26     |2017-02-27 22:22:18|1000512        |102.0              |197.59  |105.0     |101.2 |1903.1 |30.9375|-88.19083333333333|63.69   |\n",
      "|26     |2017-02-27 23:36:32|1000512        |91.0               |197.12  |114.0     |112.94|1913.19|30.9375|-86.95361111111112|50.64   |\n",
      "|26     |2017-02-27 23:16:55|1000512        |95.0               |196.51  |118.0     |98.48 |1904.8 |30.9375|-87.28055555555555|60.9    |\n",
      "|26     |2017-02-27 23:14:09|1000512        |92.0               |189.35  |121.0     |105.41|1897.2 |30.9375|-87.32666666666667|63.78   |\n",
      "|26     |2017-02-27 22:52:32|1000512        |78.0               |201.28  |109.0     |93.17 |1915.83|30.9375|-87.68694444444445|71.84   |\n",
      "|26     |2017-02-27 22:10:49|1000512        |92.0               |206.53  |108.0     |104.09|1915.13|30.9375|-88.38222222222223|58.25   |\n",
      "|26     |2017-02-27 22:44:02|1000512        |96.0               |193.49  |107.0     |106.0 |1909.25|30.9375|-87.8286111111111 |74.13   |\n",
      "|26     |2017-02-27 22:46:48|1000512        |88.0               |201.54  |113.0     |103.88|1909.52|30.9375|-87.7825          |36.06   |\n",
      "|26     |2017-02-27 22:18:46|1000512        |92.0               |192.91  |112.0     |109.26|1915.2 |30.9375|-88.24972222222222|67.37   |\n",
      "|26     |2017-02-27 23:35:53|1000512        |100.0              |195.62  |114.0     |97.93 |1907.14|30.9375|-86.96444444444445|93.57   |\n",
      "|26     |2017-02-27 22:51:41|1000512        |92.0               |196.26  |107.0     |106.37|1908.15|30.9375|-87.70111111111112|52.75   |\n",
      "|26     |2017-02-27 23:36:31|1000512        |93.0               |187.26  |104.0     |107.8 |1914.37|30.9375|-86.9538888888889 |58.23   |\n",
      "|26     |2017-02-27 22:53:09|1000512        |90.0               |196.79  |120.0     |118.77|1906.44|30.9375|-87.67666666666668|66.18   |\n",
      "+-------+-------------------+---------------+-------------------+--------+----------+------+-------+-------+------------------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Defining the schema\n",
    "\n",
    "drive_schema = StructType([\n",
    "    StructField(\"trip_id\", StringType(), True),               # Primary key\n",
    "    StructField(\"datetime\", TimestampType(), True),           # Primary key\n",
    "    StructField(\"vehicle_spec_id\", LongType(), True),\n",
    "    StructField(\"engine_coolant_temp\", DoubleType(), True),\n",
    "    StructField(\"eng_load\", DoubleType(), True),\n",
    "    StructField(\"fuel_level\", DoubleType(), True),\n",
    "    StructField(\"iat\", DoubleType(), True),\n",
    "    StructField(\"rpm\", DoubleType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"long\", DoubleType(), True),\n",
    "    StructField(\"velocity\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# I was having issue loading all the prquet with a * wildcard so I decided to union each file into one dataframe\n",
    "\n",
    "folder = \"C:/Users/tjsde/OneDrive/Documents/Git/tech-test-data/supporting-data/drive\"\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "dfs = [spark.read.schema(drive_schema).parquet(f) for f in files]\n",
    "dfDriving = dfs[0]\n",
    "for d in dfs[1:]:\n",
    "    dfDriving = dfDriving.unionByName(d)\n",
    "\n",
    "# We create a surrogate key from the trip_id such that it's a long type (as per the final table requirments)\n",
    "# My assumption for the requirment of a long column type is to optimize the resulting table (longs are easier to process and lower in byte size)\n",
    "# We repartition and cache this table to prevent out of memory exceptions\n",
    "dfDriving = dfDriving.withColumn(\"trip_id\", dense_rank().over(Window.orderBy(\"trip_id\")).cast(\"long\")).repartition(200, \"trip_id\").cache()\n",
    "\n",
    "dfDriving.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628277d",
   "metadata": {},
   "source": [
    "# Daily trip job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d0d97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-------------------+-------------------+--------+----------+------+-------+-------+------------------+--------+-------------------+----+-------+\n",
      "|vehicle_spec_id|trip_id|datetime           |engine_coolant_temp|eng_load|fuel_level|iat   |rpm    |lat    |long              |velocity|datetime_pst       |make|model  |\n",
      "+---------------+-------+-------------------+-------------------+--------+----------+------+-------+-------+------------------+--------+-------------------+----+-------+\n",
      "|1000501        |270    |2017-01-22 17:35:25|94.0               |211.57  |100.0     |102.42|2122.19|31.4375|-81.59722222222221|72.87   |2017-01-22 09:35:25|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 17:38:52|96.0               |201.9   |119.0     |106.77|2125.4 |31.4375|-81.53972222222222|74.49   |2017-01-22 09:38:52|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:09:40|99.0               |208.91  |112.0     |107.31|2121.67|31.4375|-81.02638888888889|67.89   |2017-01-22 10:09:40|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:56:06|90.0               |204.86  |120.0     |109.11|2125.94|31.4375|-80.2525          |86.93   |2017-01-22 10:56:06|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 17:06:13|91.0               |205.67  |113.0     |107.88|2126.75|31.4375|-82.08388888888888|65.89   |2017-01-22 09:06:13|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:51:34|107.0              |207.24  |111.0     |101.54|2124.24|31.4375|-80.32805555555555|63.82   |2017-01-22 10:51:34|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 17:43:48|99.0               |209.55  |123.0     |102.65|2122.28|31.4375|-81.4575          |82.74   |2017-01-22 09:43:48|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:10:55|109.0              |208.43  |116.0     |93.9  |2120.26|31.4375|-81.00555555555556|54.46   |2017-01-22 10:10:55|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:54:22|83.0               |201.06  |118.0     |102.38|2121.07|31.4375|-80.28138888888888|72.38   |2017-01-22 10:54:22|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 17:10:44|102.0              |201.16  |99.0      |108.95|2135.57|31.4375|-82.00861111111111|80.71   |2017-01-22 09:10:44|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:29:41|92.0               |219.06  |114.0     |111.6 |2137.05|31.4375|-80.69277777777778|78.8    |2017-01-22 10:29:41|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:42:37|103.0              |208.41  |113.0     |109.32|2118.46|31.4375|-80.47722222222222|61.51   |2017-01-22 10:42:37|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:00:46|93.0               |214.83  |114.0     |105.91|2125.74|31.4375|-81.17472222222223|75.41   |2017-01-22 10:00:46|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:04:01|90.0               |210.69  |100.0     |106.94|2116.57|31.4375|-81.12055555555557|113.64  |2017-01-22 10:04:01|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:13:21|112.0              |208.06  |113.0     |101.12|2126.47|31.4375|-80.965           |51.99   |2017-01-22 10:13:21|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 17:37:38|107.0              |206.01  |106.0     |89.6  |2129.41|31.4375|-81.56027777777777|60.51   |2017-01-22 09:37:38|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:57:55|98.0               |209.99  |109.0     |108.19|2123.91|31.4375|-80.22222222222223|87.95   |2017-01-22 10:57:55|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:27:49|113.0              |210.61  |107.0     |103.87|2127.05|31.4375|-80.7238888888889 |98.85   |2017-01-22 10:27:49|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 18:56:24|101.0              |206.21  |119.0     |104.25|2128.22|31.4375|-80.2475          |70.87   |2017-01-22 10:56:24|Jeep|Compass|\n",
      "|1000501        |270    |2017-01-22 17:41:19|101.0              |215.07  |109.0     |101.84|2127.01|31.4375|-81.49888888888889|75.01   |2017-01-22 09:41:19|Jeep|Compass|\n",
      "+---------------+-------+-------------------+-------------------+--------+----------+------+-------+-------+------------------+--------+-------------------+----+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "date = \"2017/01/22\" # This is the input date in PST; we assume this format will always be followed\n",
    "\n",
    "# This updates the timestamp to be in PST from UTC\n",
    "# We filter for records only on that date\n",
    "dfDailyTrip = dfDriving.withColumn(\"datetime_pst\", from_utc_timestamp(\"datetime\", \"America/Los_Angeles\")) \\\n",
    "    .filter(to_date(col(\"datetime_pst\")) == to_date(lit(date), \"yyyy/MM/dd\"))\n",
    "\n",
    "# This adds in the make and model of the car for each trip, if unknown then it's NULL\n",
    "dfDailyTrip = dfDailyTrip.join(\n",
    "    dfVehicle.select(col(\"vehicle_spec_id\"), col(\"make\"), col(\"Model\").alias(\"model\")),\n",
    "    on=\"vehicle_spec_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dfDailyTrip.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd421d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+------------------+\n",
      "|      make|      model|vehicle_spec_id|distance_travelled|\n",
      "+----------+-----------+---------------+------------------+\n",
      "|      NULL|       NULL|        1000505| 836.2803777777787|\n",
      "|      NULL|       NULL|        1000508|470.54720277777835|\n",
      "|      NULL|       NULL|        1000516|232.36941666666593|\n",
      "|      Jeep|    Compass|        1000501| 675.5876249999998|\n",
      "|      NULL|       NULL|        1000512|28.409774999999982|\n",
      "|      NULL|       NULL|        1000513|236.59707222222238|\n",
      "|      NULL|       NULL|        1000517|245.92137499999984|\n",
      "|      NULL|       NULL|        1000509|223.45613611111085|\n",
      "|      Audi|         Q3|        1000507|196.74214166666633|\n",
      "|      NULL|       NULL|        1000510|243.43013333333352|\n",
      "|      NULL|       NULL|        1000518|160.91343888888886|\n",
      "|Volkswagen|     Passat|        1000504|189.36821944444415|\n",
      "|     Honda|      Civic|        1000500|296.58623611111034|\n",
      "|       BMW|3 Series GT|        1000506|239.04477222222175|\n",
      "|   Hyundai|      Creta|        1000502|237.62253055555607|\n",
      "+----------+-----------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate time difference in seconds between consecutive readings using a window partitioned by trip_id and ordered by datetime_pst\n",
    "# In simple terms we use the window defined with lag() to determine the diference in time between each record in hours for each trip\n",
    "dfWithTimeDiff = dfDailyTrip.withColumn(\"prev_time\", lag(\"datetime_pst\").over(Window.partitionBy(\"trip_id\").orderBy(\"datetime_pst\"))) \\\n",
    "    .withColumn(\"time_diff_hours\", \n",
    "                (unix_timestamp(\"datetime_pst\") - unix_timestamp(\"prev_time\")) / 3600) # Converts the seconds to hours\n",
    "\n",
    "# Calculate segment distance: velocity (km/h) × time_diff_hours\n",
    "# Now we know the time between each record in hours, we can multiply it by the velocity to determine the distance covered between each record for each trip\n",
    "dfWithDistance = dfWithTimeDiff.withColumn(\"segment_distance_km\", \n",
    "                                           col(\"velocity\") * col(\"time_diff_hours\"))\n",
    "\n",
    "# Gets the sum of the distance of each trip\n",
    "# This gets the distance of each trip by getting the sum of the distance covered between each record\n",
    "dfTripDistance = dfWithDistance.groupBy(\"trip_id\", \"make\", \"model\", \"vehicle_spec_id\").agg(\n",
    "    sum(\"segment_distance_km\").alias(\"trip_distance_km\")\n",
    ")\n",
    "\n",
    "# Gets the sum of the distance of every trip with each vehicle\n",
    "dfVehicleDistance = dfTripDistance.groupBy(\"make\", \"model\", \"vehicle_spec_id\").agg( \n",
    "    sum(\"trip_distance_km\").alias(\"distance_travelled\")\n",
    ")\n",
    "\n",
    "try:\n",
    "    dfVehicleDistance.show()\n",
    "except Exception as e:\n",
    "    print(\"Failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364c429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+---------------+---------------------+\n",
      "|trip_id|      make|      model|vehicle_spec_id|trip_duration_minutes|\n",
      "+-------+----------+-----------+---------------+---------------------+\n",
      "|    270|      Jeep|    Compass|        1000501|   123.66666666666667|\n",
      "|    415|      Audi|         Q3|        1000507|   109.18333333333334|\n",
      "|   1694|      NULL|       NULL|        1000512|   15.933333333333334|\n",
      "|    845|      NULL|       NULL|        1000505|                 78.5|\n",
      "|   1504|       BMW|3 Series GT|        1000506|   133.23333333333332|\n",
      "|   1048|      NULL|       NULL|        1000505|                123.8|\n",
      "|    586|      NULL|       NULL|        1000505|                 83.3|\n",
      "|    589|      NULL|       NULL|        1000517|   122.76666666666667|\n",
      "|    413|      NULL|       NULL|        1000505|   132.51666666666668|\n",
      "|    161|      NULL|       NULL|        1000505|   24.983333333333334|\n",
      "|    712|      NULL|       NULL|        1000510|    78.98333333333333|\n",
      "|    208|       BMW|3 Series GT|        1000506|                102.5|\n",
      "|   1647|      NULL|       NULL|        1000509|   115.41666666666667|\n",
      "|   1498|Volkswagen|     Passat|        1000504|                49.45|\n",
      "|   1517|      NULL|       NULL|        1000517|    61.36666666666667|\n",
      "|    838|      NULL|       NULL|        1000510|               149.45|\n",
      "|    441|      NULL|       NULL|        1000509|   0.6166666666666667|\n",
      "|     48|      NULL|       NULL|        1000516|   146.81666666666666|\n",
      "|   1013|      NULL|       NULL|        1000508|   118.26666666666667|\n",
      "|   1412|      NULL|       NULL|        1000505|   139.51666666666668|\n",
      "+-------+----------+-----------+---------------+---------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# This gets the difference in time between the first and last datetime_pst of each trip\n",
    "# This assums that every trip doesn't have multiple vehicles;\n",
    "# We could check if a trip has multiple vehicles by chaning if there are any duplicate trip_id values\n",
    "dfDailyTripDuration = dfDailyTrip.groupBy(\"trip_id\", \"make\", \"model\", \"vehicle_spec_id\").agg(\n",
    "    ((unix_timestamp(max(\"datetime_pst\")) - unix_timestamp(min(\"datetime_pst\"))) / 60).alias(\"trip_duration_minutes\") # Converts the seconds to minutes\n",
    ")\n",
    "\n",
    "dfDailyTripDuration.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bec3867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-----+---------------------+------------------+\n",
      "|date_pst  |trip_id|make   |model|trip_duration_minutes|distance_travelled|\n",
      "+----------+-------+-------+-----+---------------------+------------------+\n",
      "|2017-01-22|339    |Hyundai|Creta|75.0667              |237.6225          |\n",
      "|2017-01-22|1350   |Hyundai|Creta|71.0500              |237.6225          |\n",
      "|2017-01-22|216    |Hyundai|Creta|89.9167              |237.6225          |\n",
      "|2017-01-22|845    |NULL   |NULL |78.5000              |836.2804          |\n",
      "|2017-01-22|1048   |NULL   |NULL |123.8000             |836.2804          |\n",
      "|2017-01-22|586    |NULL   |NULL |83.3000              |836.2804          |\n",
      "|2017-01-22|413    |NULL   |NULL |132.5167             |836.2804          |\n",
      "|2017-01-22|161    |NULL   |NULL |24.9833              |836.2804          |\n",
      "|2017-01-22|1412   |NULL   |NULL |139.5167             |836.2804          |\n",
      "|2017-01-22|1188   |NULL   |NULL |136.4167             |836.2804          |\n",
      "|2017-01-22|251    |NULL   |NULL |68.0833              |836.2804          |\n",
      "|2017-01-22|1013   |NULL   |NULL |118.2667             |470.5472          |\n",
      "|2017-01-22|435    |NULL   |NULL |115.2500             |470.5472          |\n",
      "|2017-01-22|290    |NULL   |NULL |150.4667             |470.5472          |\n",
      "|2017-01-22|1564   |NULL   |NULL |54.5333              |470.5472          |\n",
      "|2017-01-22|415    |Audi   |Q3   |109.1833             |196.7421          |\n",
      "|2017-01-22|878    |Audi   |Q3   |71.0833              |196.7421          |\n",
      "|2017-01-22|48     |NULL   |NULL |146.8167             |232.3694          |\n",
      "|2017-01-22|333    |NULL   |NULL |56.2167              |232.3694          |\n",
      "|2017-01-22|1694   |NULL   |NULL |15.9333              |28.4098           |\n",
      "+----------+-------+-------+-----+---------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# We perform a join betwen the two dataframes on the vehicle_spec_id in order to return the desired table\n",
    "# Joining on vehicle_spec_id allows for the most accurate data;\n",
    "# If we were to join on make & model, there could be a case where the make and model are duplicated but the vehicle_spec_id is different because it's from a different year for example;\n",
    "# This also allows us to calculate distanct values for vehicles we don't the make and model info for\n",
    "dfDailyTripFinal = dfDailyTripDuration.join(\n",
    "    dfVehicleDistance,\n",
    "    dfDailyTripDuration.vehicle_spec_id ==  dfVehicleDistance.vehicle_spec_id,\n",
    "    how=\"left\"\n",
    ").drop(dfDailyTripDuration[\"make\"]).drop(dfDailyTripDuration[\"model\"]) # We drop duplicate columns from one side of the join\n",
    "\n",
    "# We add in the date that was passed as input as this data is processed for that date\n",
    "# We select the columns in the order & types desired\n",
    "dfDailyTripFinal = dfDailyTripFinal.withColumn(\"date_pst\", to_date(lit(date), \"yyyy/MM/dd\")) \\\n",
    "    .select(\"date_pst\", \"trip_id\", \"make\", \"model\", col(\"trip_duration_minutes\").cast(DecimalType(10, 4)), col(\"distance_travelled\").cast(DecimalType(10, 4)))\n",
    "    \n",
    "dfDailyTripFinal.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d458ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When writing the parquet, we use the input date to generate a datetime folder structure to meet the last two characteristics;\n",
    "# We could also use partitionBy() on the date_pst column when saving the parquet so that it's always organized correctly (the method I used wouldn't be accurate if we had late arriving data when processing daily for example)\n",
    "# We also ensure that overwrite is enabled so if it's ran on the same input date multiple times it will update whats written\n",
    "\n",
    "try:\n",
    "    dfDailyTripFinal.write.mode(\"overwrite\").parquet(f\"C:/Users/tjsde/OneDrive/Documents/Git/tech-test-data/written-data/{date}/\")\n",
    "except Exception as e:\n",
    "    print(\"Write failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290443a",
   "metadata": {},
   "source": [
    "# Trip SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bcaf471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|trip_id|         start_time|           end_time|\n",
      "+-------+-------------------+-------------------+\n",
      "|     26|2017-02-27 22:00:00|2017-02-27 23:50:01|\n",
      "|     29|2017-02-01 16:00:00|2017-02-01 17:04:38|\n",
      "|    474|2017-01-07 12:00:00|2017-01-07 12:31:38|\n",
      "|    964|2017-01-28 11:00:00|2017-01-28 11:45:33|\n",
      "|   1677|2017-01-01 00:00:00|2017-01-01 02:34:48|\n",
      "|   1697|2017-01-07 02:00:00|2017-01-07 04:30:21|\n",
      "|     65|2017-01-08 09:00:00|2017-01-08 10:15:07|\n",
      "|    191|2017-01-06 20:00:00|2017-01-06 21:56:59|\n",
      "|    418|2017-02-14 06:00:00|2017-02-14 06:05:15|\n",
      "|    541|2017-01-21 23:00:00|2017-01-22 00:41:55|\n",
      "|    558|2017-01-06 20:00:00|2017-01-06 21:02:40|\n",
      "|   1010|2017-01-21 18:00:00|2017-01-21 20:04:08|\n",
      "|   1224|2017-02-14 07:00:00|2017-02-14 09:05:34|\n",
      "|   1258|2017-01-06 20:00:00|2017-01-06 21:42:24|\n",
      "|   1277|2017-01-27 07:00:00|2017-01-27 08:30:32|\n",
      "|   1360|2017-02-22 23:00:00|2017-02-22 23:12:02|\n",
      "|    222|2017-01-06 23:00:00|2017-01-07 01:49:41|\n",
      "|    270|2017-01-22 17:00:00|2017-01-22 19:03:40|\n",
      "|    293|2017-01-28 14:00:00|2017-01-28 15:57:37|\n",
      "|    730|2017-01-21 13:00:00|2017-01-21 13:26:31|\n",
      "+-------+-------------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#  We create our views here to perform transformations on\n",
    "dfVehicle.createOrReplaceTempView(\"Vehicle\")\n",
    "dfDriving.createOrReplaceTempView(\"Drive\")\n",
    "\n",
    "result = spark.sql(\"\"\" \n",
    "    -- This returns the start and end times for each trip_id; \n",
    "    -- This can be used to determine the start and end fuel levels and assums that a trip doesn't include refueling stops\n",
    "    SELECT\n",
    "        trip_id,\n",
    "        MIN(datetime) AS start_time,\n",
    "        MAX(datetime) AS end_time\n",
    "    FROM Drive\n",
    "    GROUP BY trip_id\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cde8f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|trip_id|start_fuel|\n",
      "+-------+----------+\n",
      "|    505|      91.0|\n",
      "|    620|     105.0|\n",
      "|    708|    184.41|\n",
      "|    781|     111.0|\n",
      "|   1103|      85.0|\n",
      "|   1423|    148.11|\n",
      "|   1537|      96.0|\n",
      "|     36|    193.32|\n",
      "|     95|      81.0|\n",
      "|    213|    134.35|\n",
      "|    323|    179.31|\n",
      "|    574|     100.0|\n",
      "|    614|      99.0|\n",
      "|    961|    144.54|\n",
      "|   1318|      85.0|\n",
      "|   1512|     112.0|\n",
      "|     85|    133.92|\n",
      "|    125|      65.0|\n",
      "|    178|    154.98|\n",
      "|    557|     101.0|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\" \n",
    "    -- Using the previous query again we can do a join to get a table of start_fuel levels for each trip_id\n",
    "    WITH trip_times AS (\n",
    "    SELECT\n",
    "        trip_id,\n",
    "        MIN(datetime) AS start_time,\n",
    "        MAX(datetime) AS end_time\n",
    "    FROM Drive\n",
    "    GROUP BY trip_id\n",
    "    )\n",
    "                   \n",
    "    SELECT\n",
    "        d.trip_id,\n",
    "        d.fuel_level AS start_fuel\n",
    "    FROM Drive d\n",
    "    JOIN trip_times t \n",
    "        ON d.trip_id = t.trip_id AND d.datetime = t.start_time\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5245b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|trip_id|end_fuel|\n",
      "+-------+--------+\n",
      "|    113|  169.62|\n",
      "|    116|    96.0|\n",
      "|    415|    55.0|\n",
      "|    560|    90.0|\n",
      "|   1321|  141.75|\n",
      "|   1608|  146.12|\n",
      "|   1645|   105.0|\n",
      "|    528|  204.27|\n",
      "|    968|    83.0|\n",
      "|   1579|  116.19|\n",
      "|   1633|  174.52|\n",
      "|    235|  180.36|\n",
      "|    237|  194.89|\n",
      "|   1055|    67.0|\n",
      "|   1303|    72.0|\n",
      "|   1347|    82.0|\n",
      "|   1449|  187.29|\n",
      "|   1609|  136.36|\n",
      "|    357|  154.63|\n",
      "|    441|  178.92|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\" \n",
    "    -- We can do the same as above but for end_fuel\n",
    "    WITH trip_times AS (\n",
    "    SELECT\n",
    "        trip_id,\n",
    "        MIN(datetime) AS start_time,\n",
    "        MAX(datetime) AS end_time\n",
    "    FROM Drive\n",
    "    GROUP BY trip_id\n",
    "    )\n",
    "                   \n",
    "    SELECT\n",
    "        d.trip_id,\n",
    "        d.fuel_level AS end_fuel\n",
    "    FROM Drive d\n",
    "    JOIN trip_times t \n",
    "        ON d.trip_id = t.trip_id AND d.datetime = t.end_time\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11af00de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+------------------+---------------------+------------------+\n",
      "|trip_id|      make|      model|fuel_tank_capacity|average_eng_load_perc|  average_velocity|\n",
      "+-------+----------+-----------+------------------+---------------------+------------------+\n",
      "|     29|Volkswagen|     Passat|                66|    82.77660302584538| 58.03923433874713|\n",
      "|    474|Volkswagen|     Passat|                66|    76.44867164348615| 67.47900473933647|\n",
      "|   1697|      Audi|         Q3|                64|    77.64465598254368| 64.98969518953648|\n",
      "|    541|   Hyundai|      Creta|                55|     77.2560471408968| 53.84619359058186|\n",
      "|   1010|       BMW|3 Series GT|                57|    85.49964701144201| 72.07063901194786|\n",
      "|   1277|   Hyundai|      Creta|                55|    81.96017366637443| 65.74867660592673|\n",
      "|   1360|      Jeep|    Compass|                60|    74.94978981910882| 69.54193637621024|\n",
      "|    270|      Jeep|    Compass|                60|     82.3756303653381| 69.11419485244568|\n",
      "|    730|Volkswagen|     Passat|                66|    82.22541383387514| 64.33543341708544|\n",
      "|    938|      Audi|         Q3|                64|    76.11012043986732| 77.49881899109792|\n",
      "|   1371|     Honda|      Civic|                47|    75.68658956929454|  71.9457508754378|\n",
      "|   1551|      Jeep|    Compass|                60|    80.30722615240494| 51.02835255354194|\n",
      "|   1642|      Jeep|    Compass|                60|    75.30034057338791| 65.97927400468441|\n",
      "|    243|Volkswagen|     Passat|                66|    78.05140770240972|  70.8945721342437|\n",
      "|    367|       BMW|3 Series GT|                57|    74.52665793695336| 52.98797340271762|\n",
      "|    705|     Skoda|     Superb|                66|    80.78000899685125| 63.95580836707149|\n",
      "|   1202|     Skoda|     Superb|                66|    74.89646287009822| 54.03763504118094|\n",
      "|   1463|     Honda|      Civic|                47|    79.62515578058267|60.028494720063726|\n",
      "|   1532|     Honda|      Civic|                47|    81.15566083035107|53.995768745266325|\n",
      "|     54|      Audi|         Q3|                64|    82.69247612392252|58.154276729559854|\n",
      "+-------+----------+-----------+------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\" \n",
    "    -- This is the aggregation needs to provide average_eng_load_perc & average_velocity\n",
    "    -- We can use the formula in the instructions along with the AVG() method to return average_eng_load_perc\n",
    "    -- We can simply use AVG() to return the average_velocity\n",
    "    -- This also assumes that a trip only uses one make/model of vehicle and doesn't change\n",
    "    SELECT\n",
    "        d.trip_id,\n",
    "        v.make,\n",
    "        v.model,\n",
    "        v.fuel_tank_capacity,\n",
    "        AVG(100.0 * (d.eng_load / 255)) AS average_eng_load_perc,\n",
    "        AVG(d.velocity) AS average_velocity\n",
    "    FROM Drive d\n",
    "    JOIN Vehicle v \n",
    "        ON d.vehicle_spec_id = v.vehicle_spec_id\n",
    "    WHERE v.make IS NOT NULL AND v.model IS NOT NULL\n",
    "    GROUP BY d.trip_id, v.make, v.model, v.fuel_tank_capacity\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b42a5dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+----------------+---------+\n",
      "|trip_id|average_eng_load_perc|average_velocity|fuel_used|\n",
      "+-------+---------------------+----------------+---------+\n",
      "|     29|                82.78|           58.04|     1.55|\n",
      "|    474|                76.45|           67.48|    -0.89|\n",
      "|   1697|                77.64|           64.99|    -0.50|\n",
      "|    541|                77.26|           53.85|     0.43|\n",
      "|   1010|                85.50|           72.07|     0.22|\n",
      "|   1277|                81.96|           65.75|     0.65|\n",
      "|   1360|                74.95|           69.54|    -2.82|\n",
      "|    270|                82.38|           69.11|     1.65|\n",
      "|    730|                82.23|           64.34|    -0.35|\n",
      "|    938|                76.11|           77.50|     0.83|\n",
      "|   1371|                75.69|           71.95|     1.02|\n",
      "|   1551|                80.31|           51.03|     0.94|\n",
      "|   1642|                75.30|           65.98|    -3.06|\n",
      "|    243|                78.05|           70.89|     3.11|\n",
      "|    367|                74.53|           52.99|     1.34|\n",
      "|    705|                80.78|           63.96|     1.55|\n",
      "|   1202|                74.90|           54.04|    -0.26|\n",
      "|   1463|                79.63|           60.03|     0.92|\n",
      "|   1532|                81.16|           54.00|    -1.14|\n",
      "|     54|                82.69|           58.15|     1.97|\n",
      "+-------+---------------------+----------------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\" \n",
    "    -- We can use everything so far to return the final table\n",
    "    WITH trip_times AS (\n",
    "    SELECT\n",
    "        trip_id,\n",
    "        MIN(datetime) AS start_time,\n",
    "        MAX(datetime) AS end_time\n",
    "    FROM Drive\n",
    "    GROUP BY trip_id\n",
    "    ),\n",
    "\n",
    "    start_fuel AS (\n",
    "    SELECT\n",
    "        d.trip_id,\n",
    "        d.fuel_level AS start_fuel\n",
    "    FROM Drive d\n",
    "    JOIN trip_times t \n",
    "        ON d.trip_id = t.trip_id AND d.datetime = t.start_time\n",
    "    ),\n",
    "\n",
    "    end_fuel AS (\n",
    "    SELECT\n",
    "        d.trip_id,\n",
    "        d.fuel_level AS end_fuel\n",
    "    FROM Drive d\n",
    "    JOIN trip_times t \n",
    "        ON d.trip_id = t.trip_id AND d.datetime = t.end_time\n",
    "    ),\n",
    "\n",
    "    aggregated_metrics AS (\n",
    "    SELECT\n",
    "        d.trip_id,\n",
    "        v.make,\n",
    "        v.model,\n",
    "        v.fuel_tank_capacity,\n",
    "        AVG(100.0 * (d.eng_load / 255)) AS average_eng_load_perc,\n",
    "        AVG(d.velocity) AS average_velocity\n",
    "    FROM Drive d\n",
    "    JOIN Vehicle v \n",
    "        ON d.vehicle_spec_id = v.vehicle_spec_id\n",
    "    WHERE v.make IS NOT NULL AND v.model IS NOT NULL\n",
    "    GROUP BY d.trip_id, v.make, v.model, v.fuel_tank_capacity\n",
    "    )\n",
    "\n",
    "                   \n",
    "    -- For the fuel_used we get the fuel level converted to litres at the start and end of the trip and compare the difference;\n",
    "    -- This would assume the tank isn't refilled mid journey and a \"trip\" is defined as a continous journey without any stoppages\n",
    "    -- We also ROUND to 2 decimal places and CAST them to a DECIMAL type\n",
    "    SELECT\n",
    "        a.trip_id,\n",
    "        CAST(ROUND(a.average_eng_load_perc, 2) AS DECIMAL(10, 2)) AS average_eng_load_perc,\n",
    "        CAST(ROUND(a.average_velocity, 2) AS DECIMAL(10, 2)) AS average_velocity,\n",
    "        CAST(ROUND(((sf.start_fuel - ef.end_fuel) / 255.0) * a.fuel_tank_capacity, 2) AS DECIMAL(10, 2)) AS fuel_used\n",
    "    FROM aggregated_metrics a\n",
    "    JOIN start_fuel sf \n",
    "        ON a.trip_id = sf.trip_id\n",
    "    JOIN end_fuel ef \n",
    "        ON a.trip_id = ef.trip_id\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09fa55d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+----------------+---------+\n",
      "|trip_id|average_eng_load_perc|average_velocity|fuel_used|\n",
      "+-------+---------------------+----------------+---------+\n",
      "|     29|                82.78|           58.04|     1.55|\n",
      "|    474|                76.45|           67.48|    -0.89|\n",
      "|   1697|                77.64|           64.99|    -0.50|\n",
      "|    541|                77.26|           53.85|     0.43|\n",
      "|   1010|                85.50|           72.07|     0.22|\n",
      "|   1277|                81.96|           65.75|     0.65|\n",
      "|   1360|                74.95|           69.54|    -2.82|\n",
      "|    270|                82.38|           69.11|     1.65|\n",
      "|    730|                82.23|           64.34|    -0.35|\n",
      "|    938|                76.11|           77.50|     0.83|\n",
      "|   1371|                75.69|           71.95|     1.02|\n",
      "|   1551|                80.31|           51.03|     0.94|\n",
      "|   1642|                75.30|           65.98|    -3.06|\n",
      "|    243|                78.05|           70.89|     3.11|\n",
      "|    367|                74.53|           52.99|     1.34|\n",
      "|    705|                80.78|           63.96|     1.55|\n",
      "|   1202|                74.90|           54.04|    -0.26|\n",
      "|   1463|                79.63|           60.03|     0.92|\n",
      "|   1532|                81.16|           54.00|    -1.14|\n",
      "|     54|                82.69|           58.15|     1.97|\n",
      "+-------+---------------------+----------------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\" \n",
    "    -- The above code is quite inefficent with the joins so we can use FIRST_VALUE & LAST_VALUE instead of joining multiple tables\n",
    "    -- We make the same assumptions about the data as above:\n",
    "    WITH fuel_extremes AS (\n",
    "    SELECT\n",
    "      d.trip_id,\n",
    "      d.vehicle_spec_id,\n",
    "      v.make,\n",
    "      v.model,\n",
    "      v.fuel_tank_capacity,\n",
    "      d.velocity,\n",
    "      100.0 * (d.eng_load / 255) AS eng_load_perc,\n",
    "\n",
    "      -- Fuel level at start and end of trip\n",
    "      -- We use the trip_id as the partition; so it gets the first value of trip_id based on datetime\n",
    "      FIRST_VALUE(d.fuel_level) OVER (PARTITION BY d.trip_id ORDER BY d.datetime ASC) AS start_fuel_level,\n",
    "      LAST_VALUE(d.fuel_level) OVER (\n",
    "        PARTITION BY d.trip_id ORDER BY d.datetime ASC\n",
    "        -- Apply the function over all rows in the partition, regardless of the current row\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "      ) AS end_fuel_level\n",
    "\n",
    "    FROM Drive d\n",
    "    JOIN Vehicle v \n",
    "        ON d.vehicle_spec_id = v.vehicle_spec_id\n",
    "    WHERE v.make IS NOT NULL AND v.model IS NOT NULL\n",
    "  )\n",
    "\n",
    "  SELECT\n",
    "    trip_id,\n",
    "    CAST(ROUND(AVG(eng_load_perc), 2) AS DECIMAL(10, 2)) AS average_eng_load_perc,\n",
    "    CAST(ROUND(AVG(velocity), 2) AS DECIMAL(10, 2)) AS average_velocity,\n",
    "\n",
    "    -- Fuel used based on start and end fuel levels\n",
    "    CAST(ROUND(\n",
    "      ((MAX(start_fuel_level) - MAX(end_fuel_level)) / 255.0) * MAX(fuel_tank_capacity),\n",
    "      2\n",
    "    ) AS DECIMAL(10, 2)) AS fuel_used\n",
    "\n",
    "  FROM fuel_extremes\n",
    "  GROUP BY trip_id, make, model\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9194577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|datetime           |fuel_level|\n",
      "+-------------------+----------+\n",
      "|2017-01-06 23:00:00|197.03    |\n",
      "|2017-01-06 23:00:01|190.67    |\n",
      "|2017-01-06 23:00:02|193.94    |\n",
      "|2017-01-06 23:00:03|187.4     |\n",
      "|2017-01-06 23:00:04|189.89    |\n",
      "|2017-01-06 23:00:05|194.05    |\n",
      "|2017-01-06 23:00:06|188.78    |\n",
      "|2017-01-06 23:00:07|196.58    |\n",
      "|2017-01-06 23:00:08|193.97    |\n",
      "|2017-01-06 23:00:09|188.22    |\n",
      "|2017-01-06 23:00:10|192.73    |\n",
      "|2017-01-06 23:00:11|200.3     |\n",
      "|2017-01-06 23:00:12|197.34    |\n",
      "|2017-01-06 23:00:13|194.52    |\n",
      "|2017-01-06 23:00:14|201.92    |\n",
      "|2017-01-06 23:00:15|207.06    |\n",
      "|2017-01-06 23:00:16|193.81    |\n",
      "|2017-01-06 23:00:17|190.04    |\n",
      "|2017-01-06 23:00:18|196.45    |\n",
      "|2017-01-06 23:00:19|189.48    |\n",
      "+-------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\" \n",
    "    -- The output doesn't look correct for fuel_used in the last table\n",
    "    -- We can see here that the fuel_level doesn't go down as time moves forward; it moves inconsistently up and down every second which isn't how an actual journey would effect a tank of fuel\n",
    "    SELECT\n",
    "        d.datetime,\n",
    "        d.fuel_level\n",
    "    FROM Drive d\n",
    "    LEFT JOIN (\n",
    "        SELECT\n",
    "            vehicle_spec_id,\n",
    "            make,\n",
    "            Model AS model,\n",
    "            fuel_tank_capacity\n",
    "        FROM Vehicle\n",
    "    ) v ON d.vehicle_spec_id = v.vehicle_spec_id\n",
    "    WHERE d.trip_id = '1'\n",
    "    ORDER BY d.datetime ASC\n",
    "\"\"\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebba74db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+----------------+---------+\n",
      "|trip_id|average_eng_load_perc|average_velocity|fuel_used|\n",
      "+-------+---------------------+----------------+---------+\n",
      "|     29|                82.78|           58.04|    12.94|\n",
      "|    474|                76.45|           67.48|     8.69|\n",
      "|   1697|                77.64|           64.99|    10.79|\n",
      "|    541|                77.26|           53.85|     8.63|\n",
      "|   1010|                85.50|           72.07|     9.61|\n",
      "|   1277|                81.96|           65.75|     9.92|\n",
      "|   1360|                74.95|           69.54|     6.82|\n",
      "|    270|                82.38|           69.11|    11.53|\n",
      "|    730|                82.23|           64.34|     7.89|\n",
      "|    938|                76.11|           77.50|     8.83|\n",
      "|   1371|                75.69|           71.95|     6.45|\n",
      "|   1551|                80.31|           51.03|     9.41|\n",
      "|   1642|                75.30|           65.98|    12.47|\n",
      "|    243|                78.05|           70.89|    10.87|\n",
      "|    367|                74.53|           52.99|    10.06|\n",
      "|    705|                80.78|           63.96|    10.09|\n",
      "|   1202|                74.90|           54.04|    12.42|\n",
      "|   1463|                79.63|           60.03|     7.74|\n",
      "|   1532|                81.16|           54.00|     6.92|\n",
      "|     54|                82.69|           58.15|     8.56|\n",
      "+-------+---------------------+----------------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\" \n",
    "    -- Instead we could calculate the value based on the MIN & MAX fuel_level assuming this data simply has the fuel_levels in the wrong order\n",
    "    -- This would once again assume the fuel is not refilled mid-trip\n",
    "    -- The result seems more realistic assuming you use 5-10 litres every 100 KM\n",
    "    SELECT\n",
    "        d.trip_id,\n",
    "\n",
    "        -- Average engine load as percentage\n",
    "        CAST(ROUND(AVG(100.0 * (d.eng_load / 255)), 2) AS DECIMAL(10, 2)) AS average_eng_load_perc,\n",
    "\n",
    "        -- Average velocity\n",
    "        CAST(ROUND(AVG(d.velocity), 2) AS DECIMAL(10, 2)) AS average_velocity,\n",
    "\n",
    "        -- Fuel used based on max and min fuel level\n",
    "        CAST(ROUND(\n",
    "            (((MAX(d.fuel_level) - MIN(d.fuel_level)) / 255.0) * v.fuel_tank_capacity),\n",
    "            2\n",
    "        ) AS DECIMAL(10, 2)) AS fuel_used\n",
    "\n",
    "    FROM Drive d\n",
    "    JOIN Vehicle v ON d.vehicle_spec_id = v.vehicle_spec_id\n",
    "    WHERE v.make IS NOT NULL AND v.model IS NOT NULL\n",
    "    GROUP BY d.trip_id, v.make, v.model, v.fuel_tank_capacity\n",
    "\"\"\")\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
